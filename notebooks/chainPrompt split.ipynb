{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "# simple sequential chain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"14cb551b1ce5493084f689735ac91281\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://caecoai0jiaoa01.openai.azure.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DWSD0612_completo_V3.txt', 'r') as file:\n",
    "    data = file.read().strip()\n",
    "    data = ' '.join(data.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "divisions = [\"ENVIRONMENT DIVISION\", \"DATA DIVISION\",\"PROCEDURE DIVISION\"]\n",
    "data_split = data.split('DIVISION')[2:]\n",
    "for i,row in enumerate(data_split):\n",
    "    final.append(f'{divisions[i]} {row}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"EY_FSO_ChatTest\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_analysis = \"\"\"\n",
    "    Given the following Cobol code, translate it into PySpark code:\n",
    "\n",
    "    {code}\n",
    "            \n",
    "    Ensure that the PySpark code includes all the necessary processes.\n",
    "    Return Only the Pyspark Code\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"code\"],\n",
    "    template=code_analysis\n",
    ")\n",
    "\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt, output_key='explanation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for code in tqdm(final):\n",
    "    time.sleep(1)\n",
    "    response = completion(prompt=f)\n",
    "    try:\n",
    "        responses.append(response.json())\n",
    "    except:\n",
    "        responses.append(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:17<00:00, 45.69s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "responses = []\n",
    "for code in tqdm(final):\n",
    "    responses.append(chain.run(code=code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The COBOL code you\\'ve provided is mainly setting up the environment and file assignments, which doesn\\'t directly translate to PySpark code. However, I can show you how to load files (like the COBOL SELECT statement) into a PySpark environment. \\n\\n```python\\nfrom pyspark.sql import SparkSession\\n\\n# Create a SparkSession\\nspark = SparkSession.builder.appName(\\'app\\').getOrCreate()\\n\\n# Load the files into DataFrames\\nARQPARM_df = spark.read.text(\"path_to_ARQPARM_file\")\\nARQSORT_df = spark.read.text(\"path_to_ARQSORT_file\")\\nDIMESTIP_df = spark.read.text(\"path_to_DIMESTIP_file\")\\nARQFTP_df = spark.read.text(\"path_to_ARQFTP_file\")\\n```\\n\\nThis code assumes that the files are in text format. If the files are in a different format (like CSV, Parquet, etc.), you would need to use the appropriate method (e.g., `read.csv(\"file_path\")` or `read.parquet(\"file_path\")`) instead of `read.text`.\\n\\nAlso, replace `\"path_to_ARQPARM_file\"`, `\"path_to_ARQSORT_file\"`, `\"path_to_DIMESTIP_file\"`, and `\"path_to_ARQFTP_file\"` with the actual paths to your files.\\n\\nPlease note that PySpark does not have a direct equivalent for the SPECIAL-NAMES or DECIMAL-POINT IS COMMA settings in COBOL. The handling of decimal points would need to be managed within the data processing code, likely with the use of PySpark\\'s built-in functions for working with columns.',\n",
       " 'Unfortunately, direct conversion from Cobol to PySpark is not feasible as both languages are designed for different types of tasks. Cobol is a high-level language designed for business data processing while PySpark is a Python library for Apache Spark, a big data processing framework.\\n\\nHowever, the logic and flow of the Cobol program could be captured and rewritten in PySpark or Python. This would involve understanding the Cobol program, its data manipulations and business logic.\\n\\nFor example, if you have SQL queries in your Cobol program that are fetching data from a database, in PySpark you could use the PySpark SQL module to execute SQL queries.\\n\\nThis is a simple example of how you could read a file and perform operations in PySpark:\\n\\n```python\\nfrom pyspark.sql import SparkSession\\n\\n# Create a SparkSession\\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\\n\\n# Read data from a CSV file\\ndf = spark.read.csv(\"path_to_your_file.csv\", header=True, inferSchema=True)\\n\\n# Perform some data manipulations\\ndf = df.filter(df[\"column_name\"] > 0)\\n\\n# Write the data back to a CSV file\\ndf.write.csv(\"path_to_output_file.csv\")\\n```\\n\\nNote: The actual PySpark code would depend heavily on the specific operations and business logic in the original Cobol code.',\n",
       " 'Translating COBOL code to PySpark code is not straightforward due to the different nature of the languages. COBOL is an imperative, procedural and, in some versions, object-oriented language, while PySpark is a Python library for Apache Spark, a big data framework.\\n\\nHere\\'s a rough translation of the COBOL code into PySpark. This doesn\\'t cover all the logic as COBOL uses many features and functions that don\\'t have a direct equivalent in PySpark.\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col\\n\\nspark = SparkSession.builder.getOrCreate()\\n\\n# Read input data\\narqparm = spark.read.text(\"path_to_arqparm\")\\narqftp = spark.read.text(\"path_to_arqftp\")\\n\\n# Process data\\nprocessed = arqparm.filter(col(\"PERIODICIDADE\") == \\'D\\').filter(col(\"DATA-PROC\") != col(\"DATA-PROX\"))\\n\\n# Write output data\\nprocessed.write.text(\"path_to_output\")\\n\\n# Close SparkSession\\nspark.stop()\\n```\\n\\nPlease note that this is a very simplified version of your COBOL code. PySpark is mainly used to manipulate large datasets in a distributed computing environment, so certain COBOL concepts such as file I/O, subroutine calls, and conditional logic need to be translated into equivalent Spark transformations and actions.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sparks = []\n",
    "for resp in  responses:\n",
    "    code = re.findall(r'```python\\n(.*?)```', resp, re.DOTALL)[0]\n",
    "    sparks.append(f'```python\\n {code} ```') \n",
    "final_sparks = '\\n\\n\\n '.join(sparks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\n from pyspark.sql import SparkSession\\n\\n# Create a SparkSession\\nspark = SparkSession.builder.appName(\\'app\\').getOrCreate()\\n\\n# Load the files into DataFrames\\nARQPARM_df = spark.read.text(\"path_to_ARQPARM_file\")\\nARQSORT_df = spark.read.text(\"path_to_ARQSORT_file\")\\nDIMESTIP_df = spark.read.text(\"path_to_DIMESTIP_file\")\\nARQFTP_df = spark.read.text(\"path_to_ARQFTP_file\")\\n ```\\n\\n\\n ```python\\n from pyspark.sql import SparkSession\\n\\n# Create a SparkSession\\nspark = SparkSession.builder.appName(\"example\").getOrCreate()\\n\\n# Read data from a CSV file\\ndf = spark.read.csv(\"path_to_your_file.csv\", header=True, inferSchema=True)\\n\\n# Perform some data manipulations\\ndf = df.filter(df[\"column_name\"] > 0)\\n\\n# Write the data back to a CSV file\\ndf.write.csv(\"path_to_output_file.csv\")\\n ```\\n\\n\\n ```python\\n from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col\\n\\nspark = SparkSession.builder.getOrCreate()\\n\\n# Read input data\\narqparm = spark.read.text(\"path_to_arqparm\")\\narqftp = spark.read.text(\"path_to_arqftp\")\\n\\n# Process data\\nprocessed = arqparm.filter(col(\"PERIODICIDADE\") == \\'D\\').filter(col(\"DATA-PROC\") != col(\"DATA-PROX\"))\\n\\n# Write output data\\nprocessed.write.text(\"path_to_output\")\\n\\n# Close SparkSession\\nspark.stop()\\n ```'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sparks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Outputs/Cobol2Spark_29.md\", \"w\") as text_file:\n",
    "        text_file.write('\\n')\n",
    "        text_file.write(final_sparks)\n",
    "        text_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
