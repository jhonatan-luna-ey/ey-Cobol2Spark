{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import Cobol2Spark as chat\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DWSD0612_completo_V3.txt', 'r') as file:\n",
    "    data = file.read().strip()\n",
    "    data = ' '.join(data.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:06<00:00, 62.13s/it]\n"
     ]
    }
   ],
   "source": [
    "generation,code, usage = chat.cobol2spark(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32281800000000005\n"
     ]
    }
   ],
   "source": [
    "print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pyspark.sql import SparkSession\n",
      "\n",
      "# Creating Spark Session\n",
      "spark = SparkSession.builder.appName('COBOL_to_PySpark').getOrCreate()\n",
      "\n",
      "# Assigning files to variables\n",
      "ARQPARM = spark.read.text(\"path_to_ARQPARM_file\")\n",
      "ARQSORT = spark.read.text(\"path_to_ARQSORT_file\")\n",
      "DIMESTIP = spark.read.text(\"path_to_DIMESTIP_file\")\n",
      "ARQFTP = spark.read.text(\"path_to_ARQFTP_file\")\n",
      "\n",
      "\n",
      "\n",
      " from pyspark.sql import SparkSession\n",
      "spark = SparkSession.builder.appName('cobolToPyspark').getOrCreate()\n",
      "\n",
      "\n",
      "df = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
      "\n",
      "\n",
      "df.createOrReplaceTempView(\"table\")\n",
      "result = spark.sql(\"SELECT * FROM table WHERE column_name IN (875, 876, 878)\")\n",
      "\n",
      "\n",
      "\n",
      " from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import col\n",
      "\n",
      "# Create a SparkSession\n",
      "spark = SparkSession.builder.appName(\"COBOL_Translation\").getOrCreate()\n",
      "\n",
      "# Read the data from CSV file (assuming the data from `ATSAUDAO.SSCESTIPULANTE` is exported to a CSV file)\n",
      "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data.csv\")\n",
      "\n",
      "# Implement the filtering logic\n",
      "df = df.filter((col(\"ETP_RMO\").isin([875, 876, 878])) & \\\n",
      "               (col(\"ETP_AMD_CANCEL\") == 0 | col(\"ETP_AMD_CANCEL\") >= col(\"DATA_INI_SEL\")) & \\\n",
      "               (col(\"ETP_DT_INI\") <= col(\"DATA_FIM_SEL\")))\n",
      "\n",
      "# Implement the sorting logic\n",
      "df = df.sort([\"NUM_CGC_SORT\", \"DATA_CANCEL_SORT\", \"DATA_INIC_SORT\", \"COD_CIA_SORT\", \"COD_APOLICE_SORT\"])\n",
      "\n",
      "# Write the result to a new CSV file\n",
      "df.write.format(\"csv\").option(\"header\", \"true\").save(\"output.csv\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided PySpark code seems to be equivalent to the COBOL code. It reads the data from a CSV file, applies the same filtering and sorting logic as the COBOL code, and writes the result to a new CSV file.\n",
      "\n",
      "Here is the corrected PySpark code:\n",
      "\n",
      "```python\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import col\n",
      "\n",
      "# Create a SparkSession\n",
      "spark = SparkSession.builder.appName(\"COBOL_Translation\").getOrCreate()\n",
      "\n",
      "# Read the data from CSV file (assuming the data from `ATSAUDAO.SSCESTIPULANTE` is exported to a CSV file)\n",
      "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data.csv\")\n",
      "\n",
      "# Implement the filtering logic\n",
      "df = df.filter((col(\"ETP_RMO\").isin([875, 876, 878])) & \\\n",
      "               ((col(\"ETP_AMD_CANCEL\") == 0) | (col(\"ETP_AMD_CANCEL\") >= col(\"DATA_INI_SEL\"))) & \\\n",
      "               (col(\"ETP_DT_INI\") <= col(\"DATA_FIM_SEL\")))\n",
      "\n",
      "# Implement the sorting logic\n",
      "df = df.sort([\"NUM_CGC_SORT\", \"DATA_CANCEL_SORT\", \"DATA_INIC_SORT\", \"COD_CIA_SORT\", \"COD_APOLICE_SORT\"])\n",
      "\n",
      "# Write the result to a new CSV file\n",
      "df.write.format(\"csv\").option(\"header\", \"true\").save(\"output.csv\")\n",
      "```\n",
      "Please replace \"data.csv\" and \"output.csv\" with the actual paths to your input and output files.\n"
     ]
    }
   ],
   "source": [
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "divisions = [\"ENVIRONMENT DIVISION\", \"DATA DIVISION\",\"PROCEDURE DIVISION\"]\n",
    "data_split = data.split('DIVISION')[2:]\n",
    "for i,row in enumerate(data_split):\n",
    "    final.append(f'{divisions[i]} {row}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  code:\n",
    "  ```\n",
    "  {code}\n",
    "  ```\n",
    "\n",
    "  Your task is to do the following steps:\n",
    "\n",
    "  1. Analyze and comprehend the COBOL code snippet above.\n",
    "  2. Write an equivalent PySpark code.\n",
    "  3. The equivalent code should provide the exact same output as the COBOL code.\n",
    "  4. Follow best practices while crafting your code, like writing clean, well-indented, efficient, and concise code.\n",
    "  5. Pay extra attention in the every data processing steps like filtering, aggregation, and sorting.\n",
    "  6. Return only the PySpark code.\"\"\"\n",
    "usage = []\n",
    "responses = []\n",
    "for code in tqdm(final):\n",
    "    response = chat.generate(prompt=f\"\"\"Given the following code snippet. Your role will be to decipher the code, create an equivalent PySpark code with the same functionality, including all the necessary processes.\n",
    "\n",
    "code:\n",
    "```\n",
    "{code}\n",
    "```\n",
    "\n",
    "Your task is to do the following steps:\n",
    "\n",
    "1. Analyze and comprehend the COBOL code snippet above.\n",
    "2. Write an equivalent PySpark code.\n",
    "3. The equivalent code should provide the exact same output as the COBOL code.\n",
    "4. Follow best practices while crafting your code, like writing clean, well-indented, efficient, and concise code.\n",
    "5. Pay extra attention in the every data processing steps like filtering, aggregation, and sorting.\n",
    "6. Return only the PySpark code.\n",
    "\"\"\",temperature=0.4, max_tokens=1000)\n",
    "    responses.append(response)\n",
    "    usage.append(((response.usage.prompt_tokens/1000) * 0.006)+((response.usage.completion_tokens/1000) * 0.012))\n",
    "print(np.sum(usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sparks = '\\n\\n'.join(resp.choices[0].message.content for resp in responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Outputs/Cobol2Spark_teste_35.md\", \"w\") as text_file:\n",
    "    text_file.write(final_sparks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sparks = []\n",
    "for resp in  responses:\n",
    "    code = '\\n\\n'.join(re.findall(r'```python\\n(.*?)```', resp.choices[0].message.content, re.DOTALL))\n",
    "    sparks.append(code) \n",
    "final_sparks = '\\n\\n\\n '.join(sparks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_code = chat.generate(prompt=f\"\"\"Analise the following codes snippet and decipher the code.\n",
    "                            \n",
    "\n",
    "cobol code:\n",
    "```\n",
    "{data}\n",
    "```\n",
    "\n",
    "Pyspark code:\n",
    "```\n",
    "{final_sparks}\n",
    "```\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Analyze and comprehend the PySpark and cobol code snippet above.\n",
    "2. verify if in the Spark code there is any mistake or if it is not equivalent to the COBOL code.\n",
    "3. If there is any mistake, correct it.\n",
    "4. If the code is equivalent, return the PySpark code.\n",
    "5. pay extra attention to the data types and the every data processing steps like filtering, aggregation, and sorting.\n",
    "6. Return only the PySpark code.\"\"\",temperature=0.4,max_tokens=1000)\n",
    "\n",
    "print(((final_code.usage.prompt_tokens/1000) * 0.006)+((final_code.usage.completion_tokens/1000) * 0.012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Outputs/Cobol2Spark_35.md\", \"w\") as text_file:\n",
    "    text_file.write(final_code.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
