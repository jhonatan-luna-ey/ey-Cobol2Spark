{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import Cobol2Spark as chat\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DWSD0612_completo_V3.txt', 'r') as file:\n",
    "    data = file.read().strip()\n",
    "    data = ' '.join(data.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "divisions = [\"ENVIRONMENT DIVISION\", \"DATA DIVISION\",\"PROCEDURE DIVISION\"]\n",
    "data_split = data.split('DIVISION')[2:]\n",
    "for i,row in enumerate(data_split):\n",
    "    final.append(f'{divisions[i]} {row}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:59<00:00, 59.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15576600000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "usage = []\n",
    "responses = []\n",
    "for code in tqdm(final):\n",
    "    response = chat.generate(prompt=f\"\"\"Given the following code snippet. Your role will be to decipher the code, create an  PySpark code with the same functionality, including all the necessary processes.\n",
    "\n",
    "code:\n",
    "```\n",
    "{code}\n",
    "```\n",
    "\n",
    "Your task is to do the following steps:\n",
    "\n",
    "1. Analyze and comprehend the COBOL code snippet above.\n",
    "2. Write an equivalent PySpark code.\n",
    "3. The equivalent code should provide the exact same output as the COBOL code.\n",
    "4. Pay extra attention in the every data processing steps like filtering, aggregation, and sorting.\n",
    "5. Follow best practices while crafting your code, like writing clean, well-indented, efficient, and concise code.\n",
    "6. Return only the PySpark code.\n",
    "\"\"\",temperature=0.4, max_tokens=1000)\n",
    "    responses.append(response)\n",
    "    usage.append(((response.usage.prompt_tokens/1000) * 0.006)+((response.usage.completion_tokens/1000) * 0.012))\n",
    "print(np.sum(usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sparks = '\\n\\n'.join(resp.choices[0].message.content for resp in responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pyspark.sql import SparkSession\n",
      "\n",
      "# Initialize a SparkSession\n",
      "spark = SparkSession.builder \\\n",
      "    .appName('Translate COBOL to PySpark') \\\n",
      "    .getOrCreate()\n",
      "\n",
      "# Read data from files into DataFrames\n",
      "df_arqparm = spark.read.format('csv').option('header', 'true').load('ARQPARM.csv')\n",
      "df_arqsort = spark.read.format('csv').option('header', 'true').load('ARQSORT.csv')\n",
      "df_dimestip = spark.read.format('csv').option('header', 'true').load('DIMESTIP.csv')\n",
      "df_arqftp = spark.read.format('csv').option('header', 'true').load('ARQFTP.csv')\n",
      "\n",
      "\n",
      "df = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').option('delimiter', ';').option('decimal', ',').load('file.csv')\n",
      "\n",
      "\n",
      "\n",
      " from pyspark.sql import SparkSession\n",
      "\n",
      "# Create a SparkSession\n",
      "spark = SparkSession.builder.getOrCreate()\n",
      "\n",
      "# Load the data into a DataFrame\n",
      "df = spark.read.format(\"jdbc\").options(\n",
      "    url=\"jdbc:your_database_url\",\n",
      "    dbtable=\"ATSAUDAO.SSCESTIPULANTE\",\n",
      "    user=\"your_username\",\n",
      "    password=\"your_password\").load()\n",
      "\n",
      "# Perform the SELECT query\n",
      "result = df.select(\"ETP_CIA\", \"ETP_APOLICE\", \"ETP_RMO\", \"ETP_RSOCIAL\", \"ETP_CGC\", \"ETP_CODIGO_ATIV\", \"ETP_AMD_INICIO\", \"ETP_AMD_CANCEL\", \"ETP_DT_INI\", \"COPER_PLANO_SAUDE\") \\\n",
      "    .where(df.ETP_RMO.isin([875, 876, 878]))\n",
      "\n",
      "# Show the result\n",
      "result.show()\n",
      "\n",
      "\n",
      "\n",
      " from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder \\\n",
      "    .appName(\"COBOL to PySpark translation\") \\\n",
      "    .getOrCreate()\n",
      "\n",
      "\n",
      "df = spark.read.csv(\"path_to_your_file.csv\", header=True, inferSchema=True)\n",
      "\n",
      "\n",
      "df_filtered = df.filter(df['column_name'] > 0)\n",
      "\n",
      "\n",
      "df_sorted = df.sort(df['column_name'].asc())\n",
      "\n",
      "\n",
      "df.write.csv('path_to_output.csv')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(final_sparks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sparks = []\n",
    "for resp in  responses:\n",
    "    code = '\\n\\n'.join(re.findall(r'```python\\n(.*?)```', resp.choices[0].message.content, re.DOTALL))\n",
    "    sparks.append(code) \n",
    "final_sparks = '\\n\\n\\n '.join(sparks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobol_final = '\\n\\n'.join(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_code = chat.generate(prompt=f\"\"\"Analise the following codes snippet and decipher the code.\n",
    "                            \n",
    "\n",
    "cobol code:\n",
    "```\n",
    "{cobol_final}\n",
    "```\n",
    "\n",
    "Pyspark code:\n",
    "```\n",
    "{final_sparks}\n",
    "```\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Analyze and comprehend the PySpark and cobol code snippet above.\n",
    "2. verify if in the Spark code there is any mistake or if it is not equivalent to the COBOL code.\n",
    "3. If there is any mistake, correct it.\n",
    "4. If the code is missing some functionality, add it.\n",
    "5. pay extra attention to the data types and the every data processing steps like filtering, aggregation, and sorting.\n",
    "6. Return only the PySpark code.\"\"\",temperature=0.4,max_tokens=1000)\n",
    "\n",
    "print(((final_code.usage.prompt_tokens/1000) * 0.006)+((final_code.usage.completion_tokens/1000) * 0.012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_code.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
