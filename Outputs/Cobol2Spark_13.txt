Here is the conversion of the COBOL program into PySpark:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lit

# Create SparkSession
spark = SparkSession.builder.appName('DWSD0612').getOrCreate()

# Load data from files into DataFrames
arqparm_df = spark.read.text("path_to_ARQPARM_file")
arqsort_df = spark.read.text("path_to_ARQSORT_file")
dimestip_df = spark.read.text("path_to_DIMESTIP_file")
arqftp_df = spark.read.text("path_to_ARQFTP_file")

# Define the structure of the files
arqparm_schema = ["REG-PARAMETRO"]
arqsort_schema = ["NUM-CGC-SORT", "DATA-CANCEL-SORT", "DATA-INIC-SORT", "ANO-INIC-SORT", "MES-INIC-SORT", "DIA-INIC-SORT", "NUM-DV-CGC-SORT", "COD-CIA-SORT", "COD-APOLICE-SORT", "NOME-ESTIP-SORT", "COD-ATIV-SORT"]
dimestip_schema = ["REG-ESTIPULANTE"]
arqftp_schema = ["FTP-REGISTRO"]

# Apply the schema to the data
for schema, df in zip([arqparm_schema, arqsort_schema, dimestip_schema, arqftp_schema], [arqparm_df, arqsort_df, dimestip_df, arqftp_df]):
    for i, colname in enumerate(schema):
        df = df.withColumn(colname, df["value"].substr(i+1, len(colname))).drop("value")

# Load table data into DataFrame
table_df = spark.read.jdbc("jdbc:db2://localhost:50000/ATSAUDAO", "SSCESTIPULANTE", properties={"user": "username", "password": "password"})

# Filter and sort the data
filtered_df = table_df.filter((table_df["PERIODICIDADE"] == 'D') & (table_df["DATA-PROC"] != table_df["DATA-PROX"]))
sorted_df = filtered_df.sort(["NUM-CGC-SORT", "DATA-CANCEL-SORT", "DATA-INIC-SORT", "COD-CIA-SORT", "COD-APOLICE-SORT"])

# Write data to output files
sorted_df.write.text("path_to_output_ARQSORT_file")
dimestip_df.write.text("path_to_output_DIMESTIP_file")
arqftp_df.write.text("path_to_output_ARQFTP_file")

# Write header and trailer records to the REG-ESTIPULANTE file
header = spark.createDataFrame([(0, "DWSD0612", "current_date", "current_time", "DATA-INI", "DATA-FIM", "NOME-PGM")], ["TIPO-REG", "NOME-PGM", "DATA-SISTEMA", "HORA-SISTEMA", "DATA-INI", "DATA-FIM", "NOME-PGM"])
trailer = spark.createDataFrame([(9, "current_date", "current_time", dimestip_df.count())], ["TIPO-REG", "DATA-SISTEMA", "HORA-SISTEMA", "QTDE-REGISTROS"])
header.union(dimestip_df).union(trailer).write.text("path_to_REG-ESTIPULANTE_file")

# Stop the SparkSession
spark.stop()
```

Please replace `"path_to_file"` with the actual path to your file. Also, replace `"username"` and `"password"` with your actual DB2 username and password.

Note: This is a simplified version of the COBOL program and does not include all the functionalities. Some functionalities such as error handling and logging are not included. Also, the SQL operations are assumed to be standard SQL operations. If there are any DB2 specific operations, they might not be supported in PySpark.