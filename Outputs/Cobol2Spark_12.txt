Here is the equivalent Pyspark code for the given COBOL program:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, asc, desc

# Create Spark Session
spark = SparkSession.builder.appName("DWSD0612").getOrCreate()

# Load input data
arqparm = spark.read.text("path_to_arqparm_file")
arqsort = spark.read.text("path_to_arqsort_file")
dimestip = spark.read.text("path_to_dimestip_file")
arqftp = spark.read.text("path_to_arqftp_file")

# Define the structure of the data
arqparm = arqparm.select(col("value").alias("REG-PARAMETRO"))
arqsort = arqsort.select(col("value").alias("REG-ARQSORT"))
dimestip = dimestip.select(col("value").alias("REG-ESTIPULANTE"))
arqftp = arqftp.select(col("value").alias("FTP-REGISTRO"))

# Business logic and data processing
# Filter data based on business rules
filtered_data = arqparm.filter((col("ETP-AMD-CANCEL") != "") & (col("ETP-DT-INI") != ""))

# Sort data
sorted_data = filtered_data.sort(asc("NUM-CGC-SORT"), asc("DATA-CANCEL-SORT"), desc("DATA-INIC-SORT"), desc("COD-CIA-SORT"), desc("COD-APOLICE-SORT"))

# Write sorted data to output file
sorted_data.write.text("path_to_output_file")

# Close Spark Session
spark.stop()
```

Please note that this is a simplified version of the COBOL program and does not include all the details such as error handling, header and trailer records, and DB2 interactions. The actual conversion might be more complex depending on the specific business rules and requirements. Also, the paths to the input and output files need to be replaced with the actual paths.