{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Cobol2Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('CobolCode.txt', 'r') as f:\n",
    "    code = f.read().strip()\n",
    "    cobol = ' '.join(code.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta, documentation ,usage = Cobol2Spark.generate_code(cobol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import col, lit, when\n",
      "from datetime import datetime\n",
      "\n",
      "def main():\n",
      "    spark = SparkSession.builder.appName(\"DWSD0612\").getOrCreate()\n",
      "\n",
      "    # Read input files\n",
      "    arqparm_df = spark.read.csv('ARQPARM.csv', header=False)\n",
      "    arqsaudao_df = spark.read.format('jdbc').options(url='jdbc:db2://ATSAUDAO').load()\n",
      "    arqdesc_df = spark.read.format('jdbc').options(url='jdbc:db2://DBSISA.DESC').load()\n",
      "    arqmovi_df = spark.read.format('jdbc').options(url='jdbc:db2://DBSISA.MOVI').load()\n",
      "\n",
      "    # Data processing steps\n",
      "    # Assuming the data processing logic is to filter and sort the data based on certain conditions\n",
      "    # and then write the output to a file. The actual logic will depend on the specific requirements\n",
      "    # and the structure of the input data.\n",
      "\n",
      "    # Filter ATSAUDAO data based on RMO values\n",
      "    filtered_atetp_df = arqsaudao_df.filter(col(\"ETP_RMO\").isin([875, 876, 878]))\n",
      "\n",
      "    # Sort the data based on certain keys\n",
      "    sorted_df = filtered_atetp_df.sort(col(\"NUM-CGC-SORT\"), col(\"DATA-CANCEL-SORT\").desc(), col(\"DATA-INIC-SORT\"), col(\"COD-CIA-SORT\"), col(\"COD-APOLICE-SORT\"))\n",
      "\n",
      "    # Write the output files\n",
      "    sorted_df.write.csv('sorted_output.csv')\n",
      "\n",
      "    # Close the Spark session\n",
      "    spark.stop()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "Please note that the above code is a simplified representation of the COBOL code provided. The actual PySpark code will depend on the specific requirements, the structure of the input data, and the desired output. The COBOL code includes a lot of file handling, data definitions, and SQL operations that are not directly translatable to PySpark without knowing the schema of the data and the database connection details. The code provided is a starting point and will need to be adjusted based on the actual data processing logic required.\n"
     ]
    }
   ],
   "source": [
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```markdown\n",
      "# PySpark Code Documentation\n",
      "\n",
      "The following documentation explains the PySpark code provided between the delimiters `=== PySpark START ===` and `=== PySpark END ===`.\n",
      "\n",
      "## Import Statements\n",
      "\n",
      "```python\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import col, lit, when\n",
      "from datetime import datetime\n",
      "```\n",
      "\n",
      "- `SparkSession`: The entry point to programming Spark with the Dataset and DataFrame API.\n",
      "- `col`, `lit`, `when`: Functions from PySpark SQL used to refer to a DataFrame column, create a new column with a literal value, and implement conditional expressions, respectively.\n",
      "- `datetime`: A module from Python's standard library to work with dates and times.\n",
      "\n",
      "## Main Function\n",
      "\n",
      "```python\n",
      "def main():\n",
      "```\n",
      "\n",
      "Defines the main function that will be executed when the script is run.\n",
      "\n",
      "### Spark Session Creation\n",
      "\n",
      "```python\n",
      "    spark = SparkSession.builder.appName(\"DWSD0612\").getOrCreate()\n",
      "```\n",
      "\n",
      "Initializes a `SparkSession` with the application name \"DWSD0612\". If a session with this name already exists, it will reuse that session; otherwise, it will create a new one.\n",
      "\n",
      "### Reading Input Files\n",
      "\n",
      "```python\n",
      "    arqparm_df = spark.read.csv('ARQPARM.csv', header=False)\n",
      "    arqsaudao_df = spark.read.format('jdbc').options(url='jdbc:db2://ATSAUDAO').load()\n",
      "    arqdesc_df = spark.read.format('jdbc').options(url='jdbc:db2://DBSISA.DESC').load()\n",
      "    arqmovi_df = spark.read.format('jdbc').options(url='jdbc:db2://DBSISA.MOVI').load()\n",
      "```\n",
      "\n",
      "- Reads a CSV file named \"ARQPARM.csv\" without a header into a DataFrame `arqparm_df`.\n",
      "- Reads data from a JDBC source with specified URLs into DataFrames `arqsaudao_df`, `arqdesc_df`, and `arqmovi_df`. These URLs point to tables in a DB2 database, and the actual table names are assumed to be `ATSAUDAO`, `DESC`, and `MOVI`, respectively.\n",
      "\n",
      "### Data Processing Steps\n",
      "\n",
      "```python\n",
      "    filtered_atetp_df = arqsaudao_df.filter(col(\"ETP_RMO\").isin([875, 876, 878]))\n",
      "```\n",
      "\n",
      "Filters the `arqsaudao_df` DataFrame to only include rows where the value of the \"ETP_RMO\" column is one of 875, 876, or 878. The result is stored in `filtered_atetp_df`.\n",
      "\n",
      "```python\n",
      "    sorted_df = filtered_atetp_df.sort(col(\"NUM-CGC-SORT\"), col(\"DATA-CANCEL-SORT\").desc(), col(\"DATA-INIC-SORT\"), col(\"COD-CIA-SORT\"), col(\"COD-APOLICE-SORT\"))\n",
      "```\n",
      "\n",
      "Sorts the `filtered_atetp_df` DataFrame based on multiple columns. The sorting is primarily by \"NUM-CGC-SORT\", then by \"DATA-CANCEL-SORT\" in descending order, followed by \"DATA-INIC-SORT\", \"COD-CIA-SORT\", and \"COD-APOLICE-SORT\". The sorted DataFrame is stored in `sorted_df`.\n",
      "\n",
      "### Writing Output Files\n",
      "\n",
      "```python\n",
      "    sorted_df.write.csv('sorted_output.csv')\n",
      "```\n",
      "\n",
      "Writes the `sorted_df` DataFrame to a CSV file named \"sorted_output.csv\".\n",
      "\n",
      "### Closing Spark Session\n",
      "\n",
      "```python\n",
      "    spark.stop()\n",
      "```\n",
      "\n",
      "Stops the Spark session to free up resources.\n",
      "\n",
      "## Entry Point\n",
      "\n",
      "```python\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "Checks if the script is being run directly (as opposed to being imported as a module) and if so, calls the `main` function.\n",
      "\n",
      "## Additional Notes\n",
      "\n",
      "The provided code is a simplified representation of a COBOL codebase. The actual PySpark code will depend on specific requirements, the structure of the input data, and the desired output. The COBOL code includes file handling, data definitions, and SQL operations that are not directly translatable to PySpark without knowing the schema of the data and the database connection details. The code provided is a starting point and will need to be adjusted based on the actual data processing logic required.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18329\n"
     ]
    }
   ],
   "source": [
    "print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
